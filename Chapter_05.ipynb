{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rapid-equity",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Chapter 5 Notes\n",
    "\n",
    "### Introduction:\n",
    "\n",
    "When dealing with sequence of random variables, there is a need to clarify the different types of convergence. This chapter serves this purpose. It revolves around the two concepts of: <br> \n",
    "1. Law of large numbers\n",
    "2. Central limit theorem. \n",
    "\n",
    "### Type of Convergence:\n",
    "\n",
    "1. <b>$X_n$ converges to $X$ in probability</b>, written $X_n \\xrightarrow[]{P} X$, when: $$\\mathbb{P}(\\mid X_n - X\\mid > \\epsilon) = 0$$ for some large value of $n$.\n",
    "2. <b>$X_n$ converges to $X$ in distribution</b>, written $X_n \\rightsquigarrow X$, if: \n",
    "$$ \\lim_{n\\rightarrow\\infty}F_n(t) = F(t)$$\n",
    "fo all values of t for which $F(t)$ is continuous. \n",
    "3. <b> $X_n$ converges to $X$ in quadratic means</b>, written $X_n \\xrightarrow[]{qm}X$ if \n",
    "$$\\mathbb{E}(X_n - X)^2 \\rightarrow 0$$\n",
    "as $n \\rightarrow \\infty$\n",
    "\n",
    "### Relation between types of convergence:\n",
    "$X_n \\xrightarrow[]{qm}X \\implies X_n \\xrightarrow[]{P} X$ <br> \n",
    "$X_n \\xrightarrow[]{P} X \\implies X_n \\rightsquigarrow X$ <br> \n",
    "if $X_n \\xrightarrow[]{P} X \\implies X_n \\rightsquigarrow X$ and $\\mathbb{P}(X=c) = 1$, then $X_n \\xrightarrow[]{P} X$ <br> \n",
    "\n",
    "### More properties on the convergence of random variables: \n",
    "Convergence in probability is applicable to both the sum and product of RVs: <br>\n",
    "(a) $X_n \\xrightarrow[]{P} X$ and $Y_n \\xrightarrow[]{P} Y \\implies X_n+Y_n \\xrightarrow[]{P} X+Y$  \n",
    "(d) $X_n \\xrightarrow[]{P} X$ and $Y_n \\xrightarrow[]{P} Y \\implies X_nY_n \\xrightarrow[]{P} XY$  \n",
    "\n",
    "Convergence in distribution is applicable to both the sum and product of two RVs, provided one of them is a constant. <br>\n",
    "This is also referred to as <b>Slutzky's Theorem: </b><br>\n",
    "(c) $X_n \\rightsquigarrow X$ and $Y_n \\rightsquigarrow c \\implies X_n+c \\xrightarrow[]{P} X+c$  \n",
    "(e) $X_n \\rightsquigarrow X$ and $Y_n \\rightsquigarrow c \\implies cX_n \\xrightarrow[]{P} cX$  \n",
    "\n",
    "Convergence in quadratic means is applicable to the sum of RVs: <br>\n",
    "(b) $X_n \\xrightarrow[]{qm} X$ and $Y_n \\xrightarrow[]{qm} Y \\implies X_n+Y_n \\xrightarrow[]{qm} X+Y$ \n",
    "\n",
    "Both convergence in distribution and probablity are applicable to transformation of random varibles: <br> \n",
    "(f) $X_n \\xrightarrow[]{P} X \\implies g(X_n) \\xrightarrow[]{P} g(X)$ <br> \n",
    "(f) $X_n \\rightsquigarrow X \\implies g(X_n) \\rightsquigarrow g(X)$ \n",
    "\n",
    "\n",
    "### Law of Large Numbers (LLN): \n",
    "Weak Law of large numbers:<br> \n",
    "Given the sequence of IID RVs $X_1, X_2, ... X_n$, we have that $\\bar{X_n} \\xrightarrow[]{P} \\mu$<br>\n",
    "It can easily be proved using Chebyshev's inequality. \n",
    "\n",
    "\n",
    "### Central Limit Theorem: \n",
    "The sample mean $\\bar{X_n}$ of any distribution with mean $\\mu$ and variance $\\sigma^2$ converges in probability to a normal distribution $N(0,1)$:\n",
    "\n",
    "$Z_n \\equiv\\dfrac{\\bar{X_n} - \\mu}{\\sqrt{\\mathbb{V}(\\bar{X_n})}} = \\dfrac{\\sqrt{n}(\\bar{X_n}-\\mu)}{\\sigma} \\rightsquigarrow Z$\n",
    "\n",
    "Other theorem similar to CLT. Assume the same conditions for CTL: <br>\n",
    "$\\dfrac{\\sqrt{n}(\\bar{X_n}-\\mu)}{S_n} \\rightsquigarrow N(0,1)$\n",
    "\n",
    "\n",
    "\n",
    "The Berry-Essène Inequality: Suppose $\\mathbb{E}\\mid X_1\\mid^3 < \\infty$. Then : \n",
    "\n",
    "$$\\sup_{z}\\mid\\mathbb{P}(Z_n < z) - \\Phi(z)\\mid \\le \\dfrac{33}{4}\\dfrac{\\mathbb{E}\\mid X_1 - \\mu\\mid^3}{\\sqrt{n}\\sigma^3}$$\n",
    "\n",
    "\n",
    "### The Delta Method: \n",
    "\n",
    "Suppose $\\dfrac{\\sqrt{n}(Y_n - \\mu)}{\\sigma} \\rightsquigarrow N(0,1)$ and that $g$ is a differentiable function with $g'(x) \\neq 0$, then: \n",
    "$$\\dfrac{\\sqrt{n}(g(Y_n) - g(\\mu))}{\\mid g'(\\mu)\\mid\\sigma} \\rightsquigarrow N(0,1)$$\n",
    "Or equivalently: <br> \n",
    "$$g(Y_n) \\approx N\\big(g(\\mu), (g'(\\mu))^2\\dfrac{\\sigma^2}{n}\\big) $$\n",
    "\n",
    "\n",
    "# Chapter 5 Exercises\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Exercise 5.2\n",
    "\n",
    "<b> Part one: </b> proof $\\lim_{n\\rightarrow\\infty}\\mathbb{V}(X_n) = 0$ and $\\lim_{n\\rightarrow\\infty}\\mathbb{E}(X_n) = b \\implies X_n\\xrightarrow[]{qm} b$ <br>\n",
    "\n",
    "$X_n\\xrightarrow[]{qm} b  \\equiv  \\lim_{n\\rightarrow\\infty}\\mathbb{E}(X_n - b^2) = \\lim_{n\\rightarrow\\infty}(\\mathbb{E}(X_n^2)  +\\mathbb{E}(b)^2 - 2b\\mathbb{E}(X_n)) =$\n",
    "\n",
    "$$\\lim_{n\\rightarrow\\infty}(\\mathbb{V}(X_n^2) + \\mathbb{E}(X_n)^2  +b^2 - 2b\\mathbb{E}(X_n))$$ \n",
    " \n",
    "Plugging $\\lim_{n\\rightarrow\\infty}\\mathbb{V}(X_n) = 0$ and $\\lim_{n\\rightarrow\\infty}\\mathbb{E}(X_n) = b$ in the previous equation results in 0, meaning $X_n\\xrightarrow[]{qm} b$. \n",
    "\n",
    "<b> Part two: </b> proof $X_n\\xrightarrow[]{qm} b \\implies \\lim_{n\\rightarrow\\infty}\\mathbb{V}(X_n) = 0$ and $\\lim_{n\\rightarrow\\infty}\\mathbb{E}(X_n) = b $ <br>\n",
    "\n",
    "$X_n\\xrightarrow[]{qm} b \\implies X_n \\xrightarrow[]{P}b  \\implies X_n \\rightsquigarrow b \\implies lim_{n\\rightarrow\\infty}F_{X_n}(t) = F_{b}(t)$ <br> \n",
    "$\\dots \\implies \\lim_{n\\rightarrow\\infty}\\mathbb{V}(X_n) = \\mathbb{V}(b) = 0$ <br> \n",
    "$\\dots \\implies \\lim_{n\\rightarrow\\infty}\\mathbb{E}(X_n) = \\mathbb{E}(b) = b$\n",
    "\n",
    "\n",
    "### Exercise 5.4\n",
    "\n",
    "<b> Proof $X_n$ converges in probability to X = 0: </b> <br>\n",
    "Since $X_n$ seems to heavily concentrate at 0 as $n\\rightarrow\\infty$, let's attempt to prove $X_n$ converges in probability to $X=0$ <br> \n",
    "$\\lim_{n\\rightarrow\\infty}\\mathbb{P}(\\mid X_n - X\\mid > \\epsilon) = \\lim_{n\\rightarrow\\infty}\\mathbb{P}(X_n> \\epsilon) = 1 - \\lim_{n\\rightarrow\\infty}\\mathbb{P}(X_n \\le \\epsilon) = 1 - \\lim_{n\\rightarrow\\infty}F_{X_n}(\\epsilon) = \\dots$ <br>\n",
    "\n",
    "for $\\epsilon > 1/n$ we have that $1 \\ge F_{X_n}(\\epsilon) \\ge 1-1/n^2$, hence $\\lim_{n\\rightarrow\\infty}F_{X_n}(\\epsilon) = 1$ and $\\lim_{n\\rightarrow\\infty}\\mathbb{P}(\\mid X_n - X\\mid > \\epsilon) = 0$<br>\n",
    "\n",
    "\n",
    "<b> Proof $X_n$ does not converges quadratically to any constant:  </b> <br>\n",
    "It was proved in Exercise 5.2 that $X_n\\xrightarrow[]{qm} b\\implies \\lim_{n\\rightarrow\\infty}\\mathbb{V}(X_n) = 0$ <br> \n",
    "However, we have that: <br>\n",
    "$\\lim_{n\\rightarrow\\infty}\\mathbb{V}(X_n) = \\lim_{n\\rightarrow\\infty}\\mathbb{E}((X_n - \\mathbb{E}(X))^2) = \\lim_{n\\rightarrow\\infty}\\mathbb{E}(X_n^2) = \\lim_{n\\rightarrow\\infty}(1/n^2)(1-1/n^2) + n^2(1/2^2) = \\lim_{n\\rightarrow\\infty}1/n^2-1/n^4 + 1 = 1$\n",
    "\n",
    "### Exercise 5.5\n",
    "if $X_i$ is bernoulli(p), then $X_i^2$ is also bernoulli(p) since $P(X_i = 0) = P(X_i^2 = 0) = 1-p$ and $P(X_i = 1) = P(X_i^2 = 1) = p$.   \n",
    "\n",
    "\n",
    "from CTLs: \n",
    "$\\bar{X_n} \\rightsquigarrow N(p,\\sigma^2/n) \\implies \\lim_{n\\rightarrow\\infty}\\mathbb{V}(\\bar{X_n}) = 0$ and $\\lim_{n\\rightarrow\\infty}\\mathbb{E}(\\bar{X_n}) = p$\n",
    "\n",
    "It was proved in Exercise 5.2 that $\\lim_{n\\rightarrow\\infty}\\mathbb{V}(\\bar{X_n}) = 0$ and $\\lim_{n\\rightarrow\\infty}\\mathbb{E}(\\bar{X_n}) = p \\implies \\bar{X_n}\\xrightarrow[]{qm} p$\n",
    "\n",
    "Additionally, random variable convergence theory also specifies: <br> \n",
    "$\\bar{X_n}\\xrightarrow[]{qm} p \\implies \\bar{X_n}\\xrightarrow[]{p} p$ \n",
    "\n",
    "### Exercise 5.6\n",
    "\n",
    "Recall from CTL that $\\bar{X_n} \\rightsquigarrow N(\\mu,\\sigma^2/n)$. Given $\\mu = 68$ and $\\sigma = 2.6$, we have:  <br>\n",
    "$\\mathbb{P}(\\bar{X_n}>68) = 1 - \\mathbb{P}(\\bar{X_n}\\le68) = 1 - \\dfrac{1}{2}[1+erf(\\dfrac{68-\\mu}{\\sigma\\sqrt{2}})] = 1 - \\dfrac{1}{2}erf(0) = 1/2$\n",
    "\n",
    "### Exercise 5.8\n",
    "Recall from CTL that $\\bar{X_n} \\rightsquigarrow N(\\mu,\\sigma^2/n)$. Given $\\mu = \\lambda = \\sigma^2 = 1$, we have:<br>\n",
    "\n",
    "$\\mathbb{P}(Y < 90) = \\mathbb{P}(\\sum X_i < 90) = \\mathbb{P}(\\bar{X_n} < 90/n) = \\mathbb{P}(\\bar{X_n} < 0.9) = \\dfrac{1}{2}[1+erf(\\dfrac{0.9-1}{\\sqrt{2}})] \\approx 0.46017$\n",
    "\n",
    "### Exercise 5.9\n",
    "with $\\epsilon > 0$ and $n\\ge ln(\\epsilon + 1/2)$: <br>\n",
    "$\\lim_{n\\rightarrow\\infty}\\mathbb{P}(\\mid X_n - X\\mid > \\epsilon) = \\lim_{n\\rightarrow\\infty}\\mathbb{P}(X_n = e^n) = \\lim_{n\\rightarrow\\infty}1/n = 0$\n",
    "Therefore, $X_n$ converges in probability to $X$, which makes in converge in distribution to $X$ as well.\n",
    "\n",
    "However $X_n$ does not converge in quadratic mean to $X$: <br>\n",
    "$\\lim_{n\\rightarrow\\infty}\\mathbb{E}(X_n - X)^2 = \n",
    "\\lim_{n\\rightarrow\\infty}(\\mathbb{E}(X_n^2) + \\mathbb{E}(X^2) - 2\\mathbb{E}(XX_n^2))$ where: <br> \n",
    "$\\quad \\mathbb{E}(X_n^2) = (1/n)e^{2n} + (1)^2(1-1/n)/2 +(-1)^2(1-1/n)/2 = (1/n)e^n + (1-1/n)$ <br>\n",
    "$\\quad \\mathbb{E}(X^2) = (1)^2/2 +(-1)^2/2 = 1$ <br>\n",
    "$\\quad \\mathbb{E}(X_nX) = (1/n)(-e^{n}/2 + e^{n}/2) + (1 - 1/n)((-1)^2 + (1)^2) = 2(1 - 1/n)$ \n",
    "Thus: <br> \n",
    "$\\lim_{n\\rightarrow\\infty}\\mathbb{E}(X_n - X)^2 = \\lim_{n\\rightarrow\\infty}((1/n)e^n + (1-1/n) + 1 - 4(1 - 1/n)) \\neq 0$\n",
    "\n",
    "\n",
    "\n",
    "### Exercise 5.10\n",
    "Given $t > 0$ and $f(x)$ ~ $N(0,1)$ we have that: <br>\n",
    "$\\mathbb{E}(\\mid X \\mid^k) = \\int^{\\infty}_{-\\infty}\\mid X\\mid^kf(x)dx = \\int^{-t}_{-\\infty}\\mid X\\mid^kf(x)dx + \\int^{\\infty}_{t}\\mid X\\mid^kf(x)dx = 2\\int^{\\infty}_{t}\\mid X\\mid^kf(x)dx \\ge 2\\int^{\\infty}_{t}\\mid t\\mid^kf(x)dx \\\\\n",
    " = 2\\mid t\\mid^k\\mathbb{P}(X> t) =  \\mid t\\mid^k\\mathbb{P}(\\mid X\\mid> t)$ \n",
    " \n",
    " This results in: <br> \n",
    " $\\dfrac{\\mathbb{E}(\\mid X \\mid^k)}{t^k} \\ge \\mathbb{P}(\\mid X\\mid> t)$ \n",
    "\n",
    " \n",
    "### Exercise 5.12\n",
    " \n",
    "Given $k \\in \\mathbb{N}, k \\ge 0$, we have: <br>\n",
    "$\\lim_{n\\rightarrow\\infty}\\mathbb{P}(X_n = k) = \\mathbb{P}(X = k) \\quad\\forall k \\iff \\\\\n",
    "\\sum_{i=0}^t\\lim_{n\\rightarrow\\infty}\\mathbb{P}(X_n = k) = \\sum_{i=0}^t\\mathbb{P}(X = k) \\quad\\forall t \\iff \\\\\n",
    "\\lim_{n\\rightarrow\\infty}\\sum_{i=0}^t\\mathbb{P}(X_n = k) = \\sum_{i=0}^t\\mathbb{P}(X = k) \\quad\\forall t \\iff \\\\\n",
    "\\lim_{n\\rightarrow\\infty}F_{X_n}(t) = F_X(t)\\iff \\\\ X_n \\rightsquigarrow X$\n",
    " \n",
    "\n",
    " \n",
    "### Exercise 5.13\n",
    "$X_n = n \\cdot min\\{Z_1, Z_2, ... Z_n\\} $ <br>\n",
    "$F_{X_n} = \\mathbb{P}(X_n \\le t) = \\mathbb{P}(min\\{Z_1, Z_2, ... Z_n\\} \\le t/n) = 1 - \\mathbb{P}(min\\{Z_1, Z_2, ... Z_n\\} \\ge t/n)$ <br>\n",
    "$ 1 - \\mathbb{P}(\\text{All } Z_i \\ge t/n) = 1 - (1-F_Z(t/n))^n$ <br>\n",
    "\n",
    "The next step is to take find the limit of $1 - (1-F_Z(t/n))^n$ as $n\\rightarrow\\infty$. The limit is first applied on the $log$ of the second term. Then, L'Hôpital's Rule can be applied. Recall $F_Z(t/n)$ approaches 0 as $n\\rightarrow\\infty$ and $f_Z(t/n)$ approaches $\\lambda$ as $n\\rightarrow\\infty$. This results in: <br>\n",
    " \n",
    "$\\lim_{n\\rightarrow\\infty} ln(1-F_Z(t/n))^n = \\lim_{n\\rightarrow\\infty} \\dfrac{ln(1-F_Z(t/n))}{1/n} = \\dfrac{(1-F_Z(t/n))^{-1}(-F_Z'(t/n)(-t/n^2))}{-1/n^2} = \\dfrac{-f_Z(t/n)t}{(1-F_Z(t/n))} = -\\lambda t$\n",
    "\n",
    "Thus: \n",
    "$\\lim_{n\\rightarrow\\infty}F_{X_n} = \\lim_{n\\rightarrow\\infty}(1 - (1-F_Z(t/n))^n) = 1 - \\lim_{n\\rightarrow\\infty}(1-F_Z(t/n))^n = 1 - exp(\\lim_{n\\rightarrow\\infty}ln((1-F_Z(t/n))^n)) = 1 - e^{-\\lambda t}$\n",
    "\n",
    "The CDF of an exponential function is $1 - e^{-\\lambda t}$. \n",
    "\n",
    " \n",
    "### Exercise 5.14\n",
    "Given $\\bar{X_n}$ has a finite mean $\\mu = 0.5$ and variance $\\sigma^2 = 1/12$, CTL can be used to assert: <br> \n",
    "$$\\dfrac{\\sqrt{n}(\\bar{X_n}-\\mu)}{\\sigma} \\rightsquigarrow N(0,1)$$\n",
    "\n",
    "let $g(x)=x^2$ where $g(\\mu) = g(1/2) = 1/4 \\neq 0$. Since $g(\\mu)\\neq 0$, the Delta method can be used to assert: <br>\n",
    "\n",
    "$g(X_n) \\approx N\\big(g(\\mu), (g'(\\mu))^2\\dfrac{\\sigma^2}{n}\\big) $ <br>\n",
    "$\\bar{X_n}^2 \\approx N\\big(\\dfrac{1}{4}, \\dfrac{1}{48n}\\big) $\n",
    "\n",
    "### Exercise 5.16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "placed-boxing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
