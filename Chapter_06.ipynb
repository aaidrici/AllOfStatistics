{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "compressed-orlando",
   "metadata": {},
   "source": [
    "\n",
    "# Chapter 6 Notes\n",
    "\n",
    "### Introduction \n",
    "\n",
    "\n",
    "### Parametric vs. Non-parametric \n",
    "\n",
    "A <b>statistical model</b> $\\Im$ is a set of distribution. A <b>parametric model</b> is a set $\\Im$.  can takes a finite number of parameters. In General, a parametric Model takes the form: \n",
    "$$ \\Im = \\big\\{f(x;\\theta):  \\theta \\in \\Theta \\big\\}$$\n",
    "\n",
    "where $\\theta$ is an unknown parameter (or vector of parameter) that is part of the <b>parameter space</b> $\\Theta$. \n",
    "\n",
    "If $\\theta$ is a vector parameter but we are only interested in one of its component, then the other components are called <b>nuisance</b> components. \n",
    "\n",
    "Any function of a cumulative distribution function is called a <b>statistical functional</b>\n",
    "\n",
    "\n",
    "### Fundamental concepts in Inference \n",
    "\n",
    "#### Point estimate \n",
    "<b>Point estimate</b> refers to providing a single best guess of a quantity of interest. The quantity could be a cdf <b>F</b>, pdf <b>f</b>, a parameter from a parametric model, a regression function <b>r</b>, etc. By convention, a point estimate is denoted $\\theta$, $\\hat{\\theta}$ or $\\hat{\\theta_n}$. It is a fixed and unknown quantity. It depends on the data so it is a RV.  \n",
    "\n",
    "\n",
    "Let $X_1$, $X_2$... $X_n$ be n data points for some distribution $F$. A point estimator $\\hat{\\theta_n}$ is defined as: <br> \n",
    "$$\\hat{\\theta_n} = g(X_1, X_2... X_n)$$\n",
    "The bias of an estimator is defined as: \n",
    "$$\\text{Bias}(\\hat{\\theta_n}) = \\mathbb{E}(\\hat{\\theta_n}) - \\theta$$\n",
    "An unbias estimator has a bias value of zero. <br> \n",
    "A point estimator is considered <b>consistent</b> if it converges to its true value as the data set size increases, or more formally: $$\\hat{\\theta_n} \\xrightarrow[]{P} \\theta$$ \n",
    "\n",
    "The distribution of $\\hat{\\theta_n}$ is called the <b>sampling distribution</b>. The standard distribution of $\\hat{\\theta_n}$ is called the <b>standard error</b>: \n",
    "$$\\text{se} = \\text{se}(\\hat{\\theta_n}) = \\sqrt{\\mathbb{V}(\\hat{\\theta_n})}$$\n",
    "\n",
    "The quality of a point estimate is often assessed by the the <b>mean square error</b>, or MSE defined as: \n",
    "$$\\text{MSE} = \\mathbb{E}_{\\theta}(\\hat{\\theta_n} - \\theta)^2$$\n",
    "\n",
    "##### MSE Theorem\n",
    "$$MSE = \\text{bias}^2(\\hat{\\theta_n}) + \\mathbb{V}(\\hat{\\theta_n})$$\n",
    "\n",
    "\n",
    "An estimator is <b>asymptotically normal</b> if: \n",
    "$$ \\frac{\\hat{\\theta_n} - \\theta}{\\text{se}}  \\rightsquigarrow N(0,1)$$\n",
    "\n",
    "\n",
    "#### Confidence set \n",
    "\n",
    "A 1 - $\\alpha$ <b> confidence interval </b> for a parameter $\\theta$ is an interval $C_n = (a,b)$ where $a = a(X_1,... X_n)$ and $b = b(X_1,... X_n)$ are function of the data such that: \n",
    "    \n",
    "$$\\mathbb{P}{_\\theta}(\\theta \\in C_n) \\geq 1 - \\alpha, \\text{for all } \\theta \\in \\Theta$$\n",
    "\n",
    "we call $1-\\alpha$ the <b>coverage</b> of the confidence interval. \n",
    "It is worth emphasizing that $C_n$'s bounds are RVs, but $\\theta$ is not a random variable. One way to interpret confidence interval is to associate the <b>coverage</b> with the ratio of time the generated inverval will capture the true value of $\\theta$ if the experiment is repeated over and over again.\n",
    "\n",
    "#### Theorem on Normal-based Confidence interval\n",
    "Supposed that $\\hat{\\theta_n} \\approx N(\\theta, \\hat{se}^2)$. Let $\\Phi$ be the CDF of a standard Normal and let $z_{\\alpha/2} = \\Phi^{-1}(1-\\alpha/2)$, that is, $\\mathbb{P}(Z > z_{\\alpha/2}) = \\alpha/2$ and $\\mathbb{P}(-z_{\\alpha_2} < X < z_{\\alpha/2}) = 1 - \\alpha$ where $Z \\sim N(0,1)$. Let \n",
    "\n",
    "$$C_n = (\\hat{\\theta_n} - z_{\\alpha/2}\\hat{se}, \\hat{\\theta_n} + z_{\\alpha/2}\\hat{se})$$ \n",
    "\n",
    "Then $\\mathbb{P}_{\\theta}(\\theta\\in\\Theta) \\rightarrow 1 - \\alpha$. \n",
    "\n",
    "#### Hypothesis testing \n",
    "\n",
    "Make a null hypothesis $H_0$ and an alternative hypothesis $H_1$. Reject $H_0$ is the point estimate obainted deviates too much from the null hypothesis.\n",
    "\n",
    "\n",
    "# Chapter 6 Exercises\n",
    "\n",
    "### Exercise 6.1\n",
    "\n",
    "$\\text{bias}(\\hat{\\lambda_n})= \\mathbb{E}(\\hat{\\lambda_n}) - \\lambda = \\mathbb{E}(n^{-1}\\sum_{i=1}^n X_i) - \\lambda = n^{-1}\\sum_{i=1}^n \\mathbb{E}(X_i) - \\lambda =  n^{-1}\\sum_{i=1}^n\\lambda - \\lambda = \\lambda - \\lambda = 0$\n",
    "\n",
    "$se^2(\\hat{\\lambda_n}) = \\mathbb{V}(\\hat{\\lambda_n}) = \\mathbb{E}(\\hat{\\lambda_n} - \\lambda)^2 = \\mathbb{E}(n^{-1}\\sum_{i=1}^nX_i - n^{-1}\\sum_{i=1}^n\\lambda)^2 = n^{-2}\\mathbb{E}(\\sum_{i=1}^n(X_i - \\lambda))^2 = n^{-2}\\mathbb{E}\\big(\\sum_{j=1}^n\\sum_{i=1}^n(X_i - \\lambda)(X_j - \\lambda)\\big) = n^{-2}\\sum_{j=1}^n\\sum_{i=1}^n\\mathbb{E}((X_i - \\lambda)(X_j - \\lambda)) = n^{-2}\\sum_{i=1}^n\\mathbb{E}(X_i^2 - \\lambda) = \\lambda/n $ <br>\n",
    "$\\dots se = \\sqrt{\\lambda/n}$\n",
    "\n",
    "\n",
    "$\\text{MSE}(\\lambda) = \\text{bias}^2(\\hat{\\lambda_n}) + \\mathbb{V}(\\hat{\\lambda_n}) = \\text{bias}^2(\\hat{\\lambda_n}) + \\text{se}^2(\\hat{\\lambda_n}) = \\lambda / n$\n",
    "\n",
    "### Exercise 6.2\n",
    "\n",
    "\n",
    "Let $\\theta_n = Y = \\text{max}\\{X_1, ... X_n\\}$ <br> \n",
    "$F(y) = \\mathbb{P}(Y \\leq y) = \\mathbb{P}(\\text{all} X_i \\leq y) = \\big(\\dfrac{y}{\\theta}\\big)^n \\text{  for } 0 < y < \\theta$ <br> \n",
    "$f(y) = \\dfrac{dF(y)}{dy} = n\\dfrac{y^{n-1}}{\\theta^n}$<br> \n",
    "$\\mathbb{E}(\\theta_n) = \\hat{\\theta_n} = \\mathbb{E}(Y) = \\int_{0}^{\\theta}yf(y)dy = \\int_{0}^{\\theta}n\\dfrac{y^n}{\\theta^n}dy = \\dfrac{n}{\\theta^n}\\big[\\dfrac{y^{n+1}}{n+1}\\big]^{\\theta}_0 = \\dfrac{\\theta^{n+1}}{\\theta^n}\\dfrac{n}{n+1} = \\theta\\dfrac{n}{n+1}$ <br> \n",
    "\n",
    "$\\mathbb{E}(\\theta_n^2) = \\int_{0}^{\\theta}y^2f(y)dy = \\int_{0}^{\\theta}n\\dfrac{y^{n+1}}{\\theta^n}dy  = \\dfrac{n}{\\theta^n}\\big[\\dfrac{y^{n+2}}{n+2}\\big]^{\\theta}_0 = \\theta^2\\dfrac{n}{n+2}$\n",
    "\n",
    "$\\text{bias}(\\theta_n) = \\hat{\\theta_n} - \\theta = \\theta\\big(\\dfrac{n}{n+1} - 1\\big)= \\dfrac{-\\theta}{n+1}$\n",
    "\n",
    "$se^2(\\theta_n) = \\mathbb{V}(\\theta_n) = \\mathbb{E}(\\theta_n - \\theta)^2 = \\mathbb{E}(\\theta_n^2) - \\mathbb{E}(\\theta_n)^2 = \\theta^2\\big( \\dfrac{n}{n+2} - \\dfrac{n^2}{(n+1)^2} \\big) = \\theta^2\\dfrac{2n}{(n + 2)^2}$ <br> \n",
    "$se(\\theta_n) = \\theta\\dfrac{\\sqrt{2n}}{n + 2}$\n",
    "\n",
    "$\\text{MSE}(\\theta_n) = \\text{bias}^2(\\hat{\\theta_n}) + \\mathbb{V}(\\hat{\\theta_n}) = \\text{bias}^2(\\hat{\\theta_n}) + se^2(\\hat{\\theta_n}) = \\dfrac{\\theta^2}{(n+1)^2} + \\theta^2\\dfrac{2n}{(n + 2)^2} = \\theta^2\\big(\\dfrac{1}{(n+1)^2} + \\dfrac{2n}{(n + 2)^2}\\big)$\n",
    "\n",
    "\n",
    "\n",
    "### Exercise 6.3\n",
    "\n",
    "\n",
    "let $Y = \\hat{\\theta_n} = 2\\bar{X}$ where $X_i \\sim$ Uniform(0,$\\theta$). Recall $\\mathbb{E}(X_i) = \\theta/2$ but also that $cov(X_i,X_j) = 0 \\quad\\forall i\\neq j$ since $X_i$'s are IID. <br> \n",
    "\n",
    "$\\mathbb{E}(Y) = \\mathbb{E}(2\\bar{X}) = 2\\mathbb{E}(n^{-1}\\sum_{i=1}^nX_i) = 2(n^{-1}\\sum_{i=1}^n\\mathbb{E}(X_i)) = 2n(n^{-1}\\dfrac{\\theta + 0}{2}) = \\theta$ <br>\n",
    "\n",
    "$\\text{bias}(\\hat{\\theta_n}) = \\hat{\\theta_n} - \\theta = 0$\n",
    "\n",
    "\n",
    "$\\text{es}^2(\\hat{\\theta}) = \\mathbb{V}(\\hat{\\theta_n}) \n",
    "= \\mathbb{E}(\\hat{\\theta_n} - \\theta)^2 \n",
    "= \\mathbb{E}(2\\bar{X} - \\theta)^2 \n",
    "= \\mathbb{E}(2n^{-1}\\sum_{i=1}^nX_i - 2n^{-1}(n\\theta/2))^2\n",
    "= 4n^{-2}\\mathbb{E}(\\sum_{i=1}^n(X_i - \\theta/2))^2 = \\dots\n",
    "= 4n^{-2}\\sum_{i=1}^n\\mathbb{E}(X_i - \\theta/2)^2\n",
    "= 4n^{-2}(n\\theta^2/12)\n",
    "= \\dfrac{\\theta^2}{3n}$ <br>\n",
    "Therefore, $se(\\theta_n) = \\dfrac{\\theta}{\\sqrt{3n}}$\n",
    "\n",
    "$\\text{MSE(}\\theta_n) =  \\text{bias}^2(\\hat{\\theta_n}) + se^2(\\hat{\\theta_n})= \\dfrac{\\theta^2}{3n}$ <br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-future",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
