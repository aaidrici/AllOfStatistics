{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "innovative-subcommittee",
   "metadata": {},
   "source": [
    "# Chapter 9 Notes\n",
    "\n",
    "\n",
    "### Parametric inference \n",
    "\n",
    "In parametric inference, the distribution function must be known in advanced and the parameters $\\theta$ must have a finite number of dimensions. \n",
    "\n",
    "$$\\mathfrak{L} = \\{f(x;\\theta) : \\theta \\in \\Theta\\}\\qquad \\text{ where }\\Theta\\in\\mathbb{R}^n$$\n",
    "\n",
    "Non-parmetric inference can be more useful since it requires no prior knowledge of the distribution studied. However, Parametric inference can generate closed form solution for standard error. \n",
    "\n",
    "\n",
    "In this chapter, two methods for generating <b>parametric estimators</b> are presented: \n",
    "1. Method of moments\n",
    "2. Maximum likelihood estimator (MLE)\n",
    "\n",
    "### Method of Moments \n",
    "\n",
    "For a given model $\\mathfrak{L}$ with a parameter space $\\Theta$ of dimension $k$ , the method of moments consits of solving the system of equations obtained when equating the closed-form moments about zero $\\alpha_i$ to the estimated moment about zero $\\hat{\\alpha}_i(\\hat{\\theta})$ for $i \\in \\{1,\\dots k\\}$. The value of $\\hat{\\theta}$ solving this system is called the <b>method of moment estimator</b>. The system of equation is the following: \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\dfrac{1}{n}\\sum_{i=1}^n X_i & = \\int xf(x;\\hat{\\theta})dx \\\\\n",
    "\\dfrac{1}{n}\\sum_{i=1}^n X_i^2 & = \\int x^2f(x;\\hat{\\theta})dx \\\\\n",
    "\\vdots \\\\\n",
    "\\dfrac{1}{n}\\sum_{i=1}^n X_i^k & = \\int x^kf(x;\\hat{\\theta})dx \\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "\n",
    "### Properties of the method of moment estimators \n",
    "Estimators generated with the method of moment have the following properties, provided the model meets certain conditions: \n",
    "\n",
    "1. The estimate $\\hat{\\theta}_n$ exists with a probability tending to 1.\n",
    "2. The estimate is consitent: $\\hat{\\theta}_n \\xrightarrow{P} \\theta$\n",
    "3. The estimate $\\hat{\\theta}_n$ is asymptotically normal $\\sqrt{n}(\\hat{\\theta}_n - \\theta) \\rightsquigarrow N(0,\\sum)$ <br>\n",
    "   where $\\sum = g\\mathbb{E}_{\\theta}(YY^T)g^T, Y = (X, X^2, \\dots X^k)^T, g = (g_1, \\dots g_k)$ and $g_j = \\partial\\alpha_j^{-1}(\\theta)/\\partial\\theta $ \n",
    "   \n",
    "The last properties can be used to compute the standard error and confidence interval. However, the parametric bootstrap method might be easier to use. \n",
    "\n",
    "### Maximum likelihood \n",
    "Definition of likelihood function: \n",
    "$$\\mathcal{L}_n(\\theta) = \\prod\\limits_{i=1}^{n}f(x_i; \\theta)$$\n",
    "\n",
    "log-likelihood $L_n(\\theta)$ is often used: \n",
    "$$\\mathcal{l}_n(\\theta) = \\ln(\\mathcal{L}_n(\\theta)) = \\sum\\limits_{i=1}^{n} ln(f(x_i; \\theta))$$\n",
    "\n",
    "The method of MLE consists of selecting the value $\\hat{\\theta}$ which maximizes $\\mathcal{L}_n(\\theta)$: \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Properties of Maximum likelihood \n",
    "Provided the likelihood function is well-behaved, the MLE estimators have the following properties: \n",
    "1. Consistent \n",
    "2. Asymptotically normal \n",
    "3. Equivarient \n",
    "4. \n",
    "\n",
    "### Consistency of MLE \n",
    "\n",
    "\n",
    "### Equivariance of MLE \n",
    "\n",
    "\n",
    "### Asymptotic Normality of MLE \n",
    "\n",
    "The distribution of MLEs is approximately normal, and its approximate variance can be obtained analytically: \n",
    " \n",
    "Definition of <b>Score function</b>: <br> \n",
    "$$s(X; \\theta) = \\dfrac{\\partial(ln(f(X;\\theta)))}{\\partial\\theta}$$  <br> \n",
    "Definition of <b>Fisher information</b>: <br> \n",
    "$$I_n(\\theta) = \\mathbb{V}_{\\theta}\\big(\\sum\\limits_{i=1}^{n}s(X_i;\\theta)\\big) \n",
    "= \\sum\\limits_{i=1}^{n}\\mathbb{V}_{\\theta}\\big(s(X_i;\\theta)\\big)\n",
    "$$\n",
    "\n",
    "\n",
    "It can be proved $nI(\\theta) = I_n(\\theta)$\n",
    "\n",
    "$I(\\theta) = -\\mathbb{E}_{\\theta}\\big(\\dfrac{\\partial^2 ln(f(X;\\theta))}{\\partial^2\\theta} \\big)$\n",
    "\n",
    "<b> Theorem on Asymptotic Normality of MLE </b> <br>\n",
    "Under appropriate regularity conditions, the following holds: \n",
    "$$se(\\theta) = \\sqrt{1/I_n(\\theta)} \\quad\\text{and}\\quad \\dfrac{\\hat{\\theta}_n - \\theta}{se} \\rightsquigarrow N(0,1)$$\n",
    "\n",
    "$$se(\\hat{\\theta}_n) = \\sqrt{1/I_n(\\hat{\\theta}_n)} \\implies \\dfrac{\\hat{\\theta}_n - \\theta}{\\hat{se}} \\rightsquigarrow N(0,1)$$\n",
    "\n",
    "\n",
    "### Delta Method \n",
    "\n",
    "The delta method can be used to determine the standard error of a MLE that is function of the model's parameter. \n",
    "\n",
    "<b>Theorem on the Delta Method</b> <br> \n",
    "If $\\tau = g(\\theta)$, where $g(\\theta)$ is differentiable and $g'(\\theta) \\neq 0$ then: \n",
    "$$ \\dfrac{\\hat\\tau_n - \\tau}{\\hat{se}(\\hat\\tau)} \\rightsquigarrow N(0,1)$$\n",
    "where $\\hat\\tau_n = g(\\hat\\theta_n)$ and: \n",
    "$$\\hat{se}(\\hat\\tau_n) = |g'(\\hat\\theta)|\\hat{se}(\\hat\\theta_n) $$\n",
    "\n",
    "### Multiparameter Models\n",
    "\n",
    "Definition of Fisher <b> Information matrix</b>:  \n",
    "\n",
    "$$\n",
    "I_n(\\theta) = \n",
    "\\begin{bmatrix}\n",
    "    \\mathbb{E}_\\theta(H_{11}) & \\mathbb{E}_\\theta(H_{12})  & \\dots  & \\mathbb{E}_\\theta(H_{1k}) \\\\\n",
    "    \\mathbb{E}_\\theta(H_{21}) & \\mathbb{E}_\\theta(H_{22})  & \\dots  & \\mathbb{E}_\\theta(H_{2k}) \\\\\n",
    "    \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n",
    "    \\mathbb{E}_\\theta(H_{k1}) & \\mathbb{E}_\\theta(H_{k2})  & \\dots  & \\mathbb{E}_\\theta(H_{kk})\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "    \n",
    "where $H_{i,j} = \\dfrac{\\partial^2\\mathcal{l}_n}{\\partial\\theta_i\\partial\\theta_j} = \\dfrac{\\partial^2(ln(f(X;\\theta))}{\\partial\\theta_i\\partial\\theta_j}$\n",
    "\n",
    "$J_n(\\theta) = I_n(\\theta)^{-1}$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla g = \n",
    "\\begin{bmatrix}\n",
    "    \\dfrac{\\partial g(\\theta)}{\\partial\\theta_1} \\\\\n",
    "    \\dfrac{\\partial g(\\theta)}{\\partial\\theta_2} \\\\\n",
    "    \\vdots \\\\\n",
    "    \\dfrac{\\partial g(\\theta)}{\\partial\\theta_k} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "  \n",
    "    \n",
    "$ \\hat{se}(\\hat{\\tau}) =  \\sqrt{\\hat{\\nabla}(g)^TJ_n(\\hat{\\theta})\\hat{\\nabla}(g)}$    \n",
    "    \n",
    "### Parametric boostraping \n",
    "\n",
    "\n",
    "# Chapter 9 Exercises\n",
    "\n",
    "\n",
    "### Exercise 9.1\n",
    "\n",
    "$\\dfrac{1}{n}\\sum_{i=1}^n X_i = \\alpha/\\beta \\\\\n",
    "\\dfrac{1}{n}\\sum_{i=1}^n X_i^2 = \\alpha/\\beta^2 + (\\alpha/\\beta)^2 $\n",
    "\n",
    "Merging the first equation into the second one yields: \n",
    "\n",
    "$n^{-1}\\sum_{i=1}^n X_i^2 - (\\dfrac{1}{n}\\sum_{i=1}^n X_i)^2 = \\hat{\\sigma}^2 = \\alpha/\\beta^2 $ <br> \n",
    "$n^{-1}\\sum_{i=1}^n X_i = \\hat{\\mu} = \\alpha/\\beta $\n",
    "\n",
    "$\\hat{\\mu}^2/\\hat{\\sigma}^2 = (\\alpha^2/\\beta^2)/(\\alpha/\\beta^2) = \\alpha$ <br> \n",
    "$\\hat{\\mu}/\\hat{\\sigma}^2 = (\\alpha/\\beta)/(\\alpha/\\beta^2) = \\beta$\n",
    "\n",
    "### Exercise 9.2\n",
    "\n",
    "(a)\n",
    "$\\dfrac{1}{n}\\sum_{i=1}^n X_i = \\hat{\\mu} = (a+b)/2 \\implies 2\\hat{\\mu} - a = b\\\\\n",
    "\\dfrac{1}{n}\\sum_{i=1}^n X_i^2 = \\hat{\\sigma}^2 + \\hat{\\mu}^2 = (b-a)^2/12 + (a+b)^2/4 \n",
    "= \\dfrac{1}{3}(a^2 + ab + b^2) \n",
    "= \\dfrac{1}{3}(a^2 + a(2\\hat{\\mu} - a) + (2\\hat{\\mu} - a)^2)\n",
    "= \\dfrac{1}{3}(a^2 - 2a\\hat{\\mu} + 4\\hat{\\mu}^2) \\\\\n",
    "\\implies \n",
    "0 = a^2 - 2a\\hat{\\mu} + (\\hat{\\mu}^2 - 3\\hat{\\sigma}^2) \\implies a = \\dfrac{1}{2}\\big( 2\\hat{\\mu} \\pm \\sqrt{(2\\hat{\\mu})^2 - 4(\\hat{\\mu}^2 - 3\\hat{\\sigma}^2)}\\big) \\implies a = \\hat{\\mu} \\pm \\sqrt{3}\\hat{\\sigma}$\n",
    "\n",
    "Putting this back in the first equation yields : $b = \\hat{\\mu} \\mp \\sqrt{3}\\hat{\\sigma}$. \n",
    "The reason why there are two possible anwers is because $a$ and $b$ can be swapped and their mean and moment about zero will remain the same. Hence, if we posit $a < b$, we have the final result: \n",
    "$a = \\hat{\\mu} - \\sqrt{3}\\hat{\\sigma}$ and $b = \\hat{\\mu} + \\sqrt{3}\\hat{\\sigma}$. \n",
    "\n",
    "(b)\n",
    "\n",
    "$\n",
    " \\mathcal{L}_n(\\theta)\n",
    "  \\begin{cases} \n",
    "      \\hfill  \\big(\\dfrac{1}{b-a}\\big)^n  \\hfill & a \\leq X_i \\leq b \\quad\\forall i\\\\\n",
    "      \\hfill  0        \\hfill & \\text{otherwise} \\\\\n",
    "  \\end{cases}\n",
    "$\n",
    "\n",
    "$\\dfrac{\\partial\\mathcal{L}_n(\\theta)}{\\partial a} = n(b-a)^{n-1} >  0$ <br> \n",
    "$\\dfrac{\\partial\\mathcal{L}_n(\\theta)}{\\partial b} = -n(b-a)^{n-1} <  0$\n",
    "\n",
    "Hence $\\mathcal{L}_n(\\theta)$ can be maximized by choosing the greatest value of $a$ such that $a \\leq X_i \\quad\\forall i$ and the smallest value of $b$ such that $b \\geq X_i \\quad\\forall i$. <br> \n",
    "Therefore, $\\hat{a} = \\text{min}(X_i)$ and $\\hat{b} = \\text{max}(X_i)$ \n",
    "\n",
    "\n",
    "\n",
    "(c) <br> \n",
    "$\\tau = g(a,b) =  (b+a)/2$ <br>\n",
    "given $\\hat{a} = \\text{min}(X_i)$ and $\\hat{b} = \\text{max}(X_i)$ are the MLEs of $a$ and $b$ respectively, we have\n",
    "that $\\hat{\\tau} =  g(\\hat{a},\\hat{b}) = (\\hat{b}+\\hat{a})/2$ is the MLE of $\\tau$. \n",
    "\n",
    "\n",
    "(d)\n",
    "\n",
    "$\\tilde{\\tau} = n^{-1}\\sum X_i = \\bar{X}_n$\n",
    "\n",
    "$\\text{bias}(\\tilde{\\tau})^2 = (\\mathbb{E}(\\bar{X}_n) - \\tau)^2 = (\\mathbb{E}(n^{-1}\\sum X_i) - \\tau)^2 \n",
    "= (n^{-1}(\\sum \\mathbb{E}X_i) - \\tau)^2 =(\\mu - \\tau)^2 = 0 \n",
    "$\n",
    "\n",
    "$\\text{MSE}(\\tilde{\\tau}) = \\text{bias}(\\tilde{\\tau})^2 + \\mathbb{V}(\\tilde{\\tau}) \n",
    "= \\mathbb{V}(\\tilde{\\tau})\n",
    "= \\mathbb{V}( n^{-1}\\sum X_i)\n",
    "= n^{-2}(\\sum \\mathbb{V}X_i) = n^{-1}\\mathbb{V}(X) = \\dfrac{\\sigma^2}{n} = \\dfrac{(b-a)^2}{12n}  \n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = 1\n",
    "b = 3\n",
    "mu = (b+a)/2\n",
    "n = 10 # sample size \n",
    "N = 100000 # number of trial\n",
    "theta_t = np.zeros(shape = N)\n",
    "\n",
    "for i in range(0,N): \n",
    "    sample = np.random.uniform(a,b, size = n)\n",
    "    theta_t[i] = (max(sample)+min(sample))/2\n",
    "\n",
    "param_MSE = np.mean((theta_t - mu)**2)\n",
    "non_param_MSE = (b-a)**2/12/n\n",
    "print(f\"Simulated MSE of MLE of tau = {param_MSE:.4f}\" )\n",
    "print(f\"Analytical MSE of non-parametric estimator of tau = {non_param_MSE:.4f}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-standard",
   "metadata": {},
   "source": [
    "### Exercise 9.3\n",
    "(a) <br> \n",
    "Due to the principle of equivariance, we have:  <br> \n",
    "$\\hat{\\tau} = g(\\hat{\\mu}, \\hat{\\sigma}) = \\hat{\\mu} + \\hat{\\sigma} \\mathcal{z}_{0.95}$\n",
    "\n",
    "MLE for $\\mu$ and $\\sigma$ are obtained in example 9.11: <br> \n",
    "$\\hat{\\tau} = \\bar{X}$ and $\\hat{\\sigma} = \\sqrt{n^{-1}\\sum(X_i-\\bar{X}})^2$\n",
    "\n",
    "This results in: <br> \n",
    "$\\hat{\\tau} = \\bar{X} +  \\mathcal{z}_{0.95}\\sqrt{n^{-1}\\sum(X_i-\\bar{X}})^2$\n",
    "\n",
    "(b) <br> \n",
    "\n",
    "Compute gradient of $g(\\theta)$: <br> \n",
    "$ \\nabla g = [1,  \\mathcal{z}_{0.95}]^T  =  [1,  1.6448]^T$\n",
    "\n",
    "\n",
    "Compute the Fisher information matrix for the normal distributrion: <br> \n",
    "$\\mathcal{l}_n(\\theta) = \\sum_{i=1}^n ln(f(X;\\theta)) = \\sum_{i=1}^n(ln\\big(1/(\\sigma\\sqrt{2\\pi})\\big) -\\dfrac{(x-\\mu)^2}{2\\sigma^2})$\n",
    "\n",
    "$\\mathbb{E}_{\\theta}\\big(\\dfrac{\\partial^2\\mathcal{l}_n(\\theta)}{\\partial\\mu^2}\\big)\n",
    "= \\mathbb{E}_{\\theta}\\big(\\sum_{i=1}^n -1/\\sigma^2\\big) = -n/\\sigma^2$\n",
    "\n",
    "$\\mathbb{E}_{\\theta}\\big(\\dfrac{\\partial^2\\mathcal{l}_n(\\theta)}{\\partial\\mu\\partial\\sigma}\\big)\n",
    "= \\mathbb{E}_{\\theta}\\big(\\dfrac{\\partial^2\\mathcal{l}_n(\\theta)}{\\partial\\sigma\\partial\\mu}\\big)\n",
    "= \\mathbb{E}_{\\theta}\\big(\\sum_{i=1}^n -\\dfrac{x_i-\\mu}{2\\sigma^3}\\big) = 0$\n",
    "\n",
    "$\\mathbb{E}_{\\theta}\\big(\\dfrac{\\partial^2\\mathcal{l}_n(\\theta)}{\\partial\\sigma^2}\\big)\n",
    "= \\mathbb{E}_{\\theta}\\big(\\sum_{i=1}^n  \\dfrac{1}{\\sigma^2} - \\dfrac{3(x-\\mu)^2}{\\sigma^4} \\big) =-2n/\\sigma^2\n",
    "$\n",
    "\n",
    "$ J_n(\\theta) = I_n(\\theta)^{-1} = \n",
    "\\begin{bmatrix}\n",
    "    n/\\sigma^2 & 0 \\\\\n",
    "    0 &  2n/\\sigma^2\n",
    "\\end{bmatrix}^{-1}\n",
    "= \n",
    "\\begin{bmatrix}\n",
    "    \\sigma^2/n & 0 \\\\\n",
    "    0 &  \\sigma^2/2n\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Compute the approximate standard error: <br> \n",
    "$ \\hat{se}(\\hat{\\tau}) =  \\sqrt{\\hat{\\nabla}(g)^TJ_n(\\hat{\\theta})\\hat{\\nabla}(g)}\n",
    "= \\sqrt{\\hat{\\sigma}^2/n + \\mathcal{z}_{0.95}^2\\hat{\\sigma}^2/2n}\n",
    "=  \\sqrt{\\hat{\\sigma}^2/n  \\big(1 + \\mathcal{z}_{0.95}^2/2 \\big)}\n",
    "$    \n",
    "\n",
    "$1-\\alpha$ Confidence interval for $\\tau$: <br>\n",
    "$\\hat{\\tau} \\pm \\mathcal{z}_{1-\\alpha/2}\\hat{se}(\\hat{\\theta})$ <br> \n",
    "$\\hat{\\tau} \\pm \\mathcal{z}_{1-\\alpha/2}\\sqrt{\\hat{\\sigma}^2/n  \\big(1 + \\mathcal{z}_{0.95}^2/2 \\big)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "dated-closing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE of tau: 4.1804\n",
      "Std. err. using delta method: 0.5576\n",
      "Std. err. using parametric bootstrap: 0.5560\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "X = [3.23,-2.5,1.88,-0.68,4.43,0.17,1.03,-0.07,-0.01,0.76,1.76,3.18,0.33,-0.31,0.3,-0.61,1.52,5.43,1.54,2.28,0.42,2.33,-1.03,4, 0.39]\n",
    "n = len(X)\n",
    "X = np.array(X)\n",
    "z_0p95 = norm.ppf(0.95)\n",
    "\n",
    "tau_hat = np.mean(X) + z_0p95 * np.sqrt(np.mean((X-np.mean(X))**2))\n",
    "sig2_hat = np.mean((X-np.mean(X))**2)\n",
    "se_hat = np.sqrt(sig2_hat/n * (1 + z_0p95**2/2))\n",
    "\n",
    "\n",
    "print(f\"MLE of tau: {tau_hat:.4f}\")\n",
    "print(f\"Std. err. using delta method: {se_hat:.4f}\")\n",
    "\n",
    "\n",
    "# Parametric bootstrap \n",
    "B =100\n",
    "tau = np.empty(shape = B)\n",
    "\n",
    "for i in range(0,B): \n",
    "    sample_id = np.random.randint(n, size=n)\n",
    "    sample = np.take(X,sample_id)\n",
    "    mu_hat = np.mean(sample)\n",
    "    var_hat = np.var(sample)\n",
    "    tau[i] = mu_hat + np.sqrt(var_hat)\n",
    "    \n",
    "se_hat_boostrap = np.sqrt(np.var(tau))\n",
    "print(f\"Std. err. using parametric bootstrap: {se_hat_boostrap:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-cooperation",
   "metadata": {},
   "source": [
    "### Exercise 9.4 \n",
    "\n",
    "\n",
    "It was shown in exercise 9.2 (b) that the MLE $\\hat{\\theta}$ for the right bound $\\theta$ of a uniform distributin is max$\\{X_i\\}$. \n",
    "\n",
    "Given $\\epsilon > 0$:  <br> \n",
    "$\n",
    "\\mathbb{P}(|\\theta - \\hat{\\theta}_n| \\geq \\epsilon ) \\\\\n",
    "= \\mathbb{P}(\\theta - \\hat{\\theta}_n \\geq \\epsilon ) \\quad\\text{since } \\hat{\\theta}_n \\leq \\theta\\\\\n",
    "= \\mathbb{P}(\\hat{\\theta}_n \\leq \\theta -\\epsilon ) \\\\\n",
    "= \\prod\\limits_{i=1}^n \\mathbb{P}(X_i \\leq \\theta - \\epsilon)\n",
    "= \\big(\\dfrac{\\theta-\\epsilon}{\\theta}\\big)^n = 0 \\text{ as } n\\to\\infty$\n",
    "\n",
    "Consequently, <br>\n",
    "$\\lim_{n\\to\\infty}\\mathbb{P}(|\\theta - \\hat{\\theta}_n| \\geq \\epsilon ) = 0$ and $\\hat{\\theta}_n \\xrightarrow{P} \\theta$\n",
    "\n",
    "\n",
    "### Exercise 9.5\n",
    "\n",
    "<b>Method of moment estimator </b>: <br> \n",
    "Since there is only one parameter to consider in the poisson distribution, there is only one equation to solve for: <br> \n",
    "$n^{-1}\\sum_{i=1}^n X_i = \\lambda$ <br> \n",
    "\n",
    "\n",
    "<b>MLE method</b>: <br> \n",
    "$\\mathcal{l}_n(\\theta) = \\sum_{i=1}^n \\text{ln}\\big(\\dfrac{\\lambda^{x_i} e^{-\\lambda}}{x_i!} \\big)\n",
    "=  \\sum_{i=1}^n \\big( x_i\\text{ln}(\\lambda) -\\lambda - \\text{ln}(x_i!)\\big)\n",
    "= \\text{ln}(\\lambda)\\sum_{i=1}^n x_i - n\\lambda - \\sum_{i=1}^n\\text{ln}(x_i!)\n",
    "$\n",
    "\n",
    "$\\dfrac{\\partial\\mathcal{l}_n(\\lambda) }{\\partial\\lambda} = \\lambda^{-1}\\sum_{i=1}^nx_i - n = 0\n",
    "\\implies \\lambda = n^{-1}\\sum_{i=1}^nx_i\n",
    "$\n",
    "\n",
    "<b>Fisher information</b>: <br> \n",
    "\n",
    "$I(\\lambda) = -\\mathbb{E}_{\\lambda}(\\dfrac{\\partial^2}{\\partial\\lambda^2}\\text{ln}\\big(\\dfrac{\\lambda^{x}e^{-\\lambda}}{x!}\\big))\n",
    "= -\\mathbb{E}_{\\lambda}(\\dfrac{\\partial}{\\partial\\lambda}\\big(\\dfrac{x!}{\\lambda^{x}e^{-\\lambda}}\\dfrac{(x\\lambda^{x-1}-\\lambda^{x})e^{-\\lambda}}{x!}\\big))\n",
    "= -\\mathbb{E}_{\\lambda}(\\dfrac{\\partial}{\\partial\\lambda}(x\\lambda^{-1}-1))\n",
    "= -\\mathbb{E}_{\\lambda}(-x\\lambda^{-2})\n",
    "= \\lambda^{-2}\\mathbb{E}_{\\lambda}(x)  = \\lambda^{-1}\n",
    "$\n",
    "\n",
    "### Exercise 9.6 (Incomplete)\n",
    "\n",
    "(a) \n",
    "\n",
    "The MLE $\\hat\\psi$ of $\\psi$ can be found using the principle of equivariance by first finding the MLE of $\\theta$, then computing $\\hat\\psi$ = $g(\\hat\\theta)$: \n",
    "\n",
    "$\\frac{\\partial}{\\partial\\theta}\\big(\\sum_{i=1}^n ln f(x_i,\\theta)\\big) \n",
    "= \\frac{\\partial}{\\partial\\theta}\\big(\\sum_{i=1}^n ln(1/\\sqrt{2\\pi\\sigma^2}) + \\sum_{i=1}^n\\dfrac{-(x_i-\\theta)^2}{2\\sigma^2}\\big) \n",
    "= \\sum_{i=1}^n\\dfrac{(x_i-\\theta)}{\\sigma^2}\n",
    "= (\\sum_{i=1}^n x_i - n\\theta)/\\sigma^2 = 0 \\implies\n",
    "n^{-1}\\sum_{i=1}^n x_i = \\theta\n",
    " $\n",
    "\n",
    "<br> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(b) <br> \n",
    "The gradient method can used to determine the standard error on $\\hat\\psi$:  <br> \n",
    "$\\hat{se}(\\hat\\psi_n) = |g'(\\hat\\mu)|\\hat{se}(\\hat\\mu_n) $\n",
    "\n",
    "Differentiating $g(\\mu)$ can be done using the inverse function theorem:  \n",
    "$|g'(\\mu)| = |(\\Phi^{-1})'(\\mu)| = \\big|\\dfrac{1}{\\Phi'(\\Phi(\\mu))}\\big| = \\sigma\\sqrt{2\\pi} exp\\big(\\dfrac{(\\Phi(\\mu)-\\mu)^2}{2\\sigma^2}\\big)$\n",
    "\n",
    "The MLE standard error for a Bernoulli distribution's mean is: <br> \n",
    "$f(X;\\mu) = p^x(1-p)^{x-1}$ <br> \n",
    "$\\text{ln}(f(X;\\theta)) = x\\text{ln}(p) + (x-1)\\text{ln}(1-p)$ <br> \n",
    "$I_n(\\mu)= nI(\\mu) = -n\\mathbb{E}_{\\mu}\\big(\\dfrac{\\partial^2 \\text{ln}f(X;\\mu)}{\\partial\\mu^2}\\big) = -n\\mathbb{E}_{\\mu}\\big(\\dfrac{\\partial^2 \\text{ln}f(X;\\mu)}{\\partial\\mu^2}\\big)$\n",
    "\n",
    "$\\hat{se}(\\hat\\mu) = \\sqrt{1/I_n(\\hat\\mu)} = \\sigma/\\sqrt{n} = 1/\\sqrt{n}$\n",
    "\n",
    "The results confidence interval is: \n",
    "\n",
    "\n",
    "\n",
    "### Exercise 9.7 \n",
    "\n",
    "\n",
    "\n",
    "(a) <br> \n",
    "Let $f(x_1,x_2; p_1, p_2) = f_1(x_1; p_1)f_2(x_2; p_2) = {n_1 \\choose x_1}p_1^{x_1}(1-p_1)^{n_1-x_1}{n_2 \\choose x_2}p_2^{x_2}(1-p_2)^{n_2-x_2}$ \n",
    "\n",
    "$\\dfrac{\\partial \\text{ln}f(x_1, x_2;p_1,p_2)}{\\partial p_1} \n",
    "= \\dfrac{\\partial}{\\partial p_1}\\big(ln{n_1 \\choose x_1} + x_1 ln(p_1) + (n_1-x_1)ln(1-p_1) + ln {n_2 \\choose x_2} + x_2 ln(p_2) + (n_2-x_2)ln(1-p_2)\\big)\n",
    "=\\dfrac{x_1}{p_1} - \\dfrac{(n_1-x_1)}{1-p_1} = 0 \\implies x_1(1-p_1) = p_1(n_1-x_1) \\implies x_1  = p_1 n_1 \\implies x_1/n_1  = \\hat p_1  \n",
    "$\n",
    "\n",
    "$\\dfrac{\\partial \\text{ln}f(x_1, x_2;p_1,p_2)}{\\partial p_2} = 0 \\implies\n",
    " \\dots \\implies x_2/n_2  = \\hat p_2  \n",
    "$\n",
    "\n",
    "Using the priciple of equivariance, we have $\\psi = p_1 - p_2 \\implies \\hat\\psi = \\hat{p_1}- \\hat{p_2} = x_1/n_1 - x_2/n_2$ \n",
    "\n",
    "\n",
    "(b) <br> \n",
    "\n",
    "\n",
    "$-\\mathbb{E}\\dfrac{\\partial^2 \\text{ln}f(x_1, x_2;p_1,p_2)}{\\partial p_1\\partial p_2} \n",
    "= -\\mathbb{E}\\dfrac{\\partial^2 \\text{ln}f(x_1, x_2;p_1,p_2)}{\\partial p_2\\partial p_1}\n",
    "= 0  \n",
    "$\n",
    "\n",
    "$-\\mathbb{E}\\dfrac{\\partial^2 \\text{ln}f(x_1, x_2;p_1,p_2)}{\\partial p_1^2} \n",
    "= -\\mathbb{E}\\big(-\\dfrac{x_1}{p_1^2} - \\dfrac{(n_1-x_1)}{(1-p_1)^2}\\big)  \n",
    "= \\dfrac{n_1 p_1}{p_1^2} + \\dfrac{(n_1-n_1p_1)}{(1-p_1)^2}  \n",
    "= \\dfrac{n_1}{p_1} + \\dfrac{n_1}{(1-p_1)} \n",
    "= \\dfrac{n_1}{p_1(1-p_1)} \n",
    "$\n",
    "\n",
    "$-\\mathbb{E}\\dfrac{\\partial^2 \\text{ln}f(x_1, x_2;p_1,p_2)}{\\partial p_2^2} \n",
    "= \\dots  \n",
    "= \\dfrac{n_2}{p_2(1-p_2)} \n",
    "$\n",
    "\n",
    "\n",
    "$$\n",
    "I_n(p_1, p_2) = \n",
    "\\begin{bmatrix}\n",
    "     \\dfrac{n_1}{p_1(1-p_1)} & 0 \\\\\n",
    "    0 & \\dfrac{n_2}{p_2(1-p_2)} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "(c) <br> \n",
    "  \n",
    "$\\psi = g(p_1, p_2) = p_1 - p_2$ <br> \n",
    "$\\nabla(g) = [1, -1]^T$ <br> \n",
    "$\n",
    "J_n(\\hat p_1, \\hat p_2) = I_n(p_1, p_2)^{-1} = \n",
    "\\begin{bmatrix}\n",
    "     \\dfrac{\\hat p_1(1-\\hat p_1)}{n_1} & 0 \\\\\n",
    "    0 & \\dfrac{\\hat p_2(1-\\hat p_2)}{n_2} \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "\n",
    "$ \\hat{se}(\\hat\\psi) =  \\sqrt{\\hat{\\nabla}(g)^TJ_n(\\hat{\\theta})\\hat{\\nabla}(g)} = \\sqrt{\\dfrac{\\hat p_1(1-\\hat p_1)}{n_1} + \\dfrac{\\hat p_2(1-\\hat p_2)}{n_2}} $   \n",
    "\n",
    "\n",
    "(d) <br> \n",
    "\n",
    "(i) Delta method: <br> \n",
    "$\\hat{p_1} = 160/200 = 0.80$ <br> \n",
    "$\\hat{p_1} = 148/200 = 0.74$ <br> \n",
    "\n",
    "$C_n = \\hat\\psi \\pm \\mathcal{z}_{0.95}\\hat{se}(\\hat\\psi) =  (\\hat{p_1} - \\hat{p_2}) \\pm \\mathcal{z}_{0.95}\\sqrt{\\dfrac{\\hat p_1(1-\\hat p_1)}{n_1} + \\dfrac{\\hat p_2(1-\\hat p_2)}{n_2}}\n",
    "= 0.06 \\pm (1.2815)(0.041976) = 0.06 \\pm 0.05379 = [0.00621, 0.11379]$\n",
    "\n",
    "\n",
    "\n",
    "(ii) Parametric bootstrap : <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "adolescent-visibility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% confidence interval using parametric boostrap:\n",
      "[-0.0050,0.1300]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "B = 1000\n",
    "\n",
    "n_1 = 200; \n",
    "n_2 = 200; \n",
    "p_1_hat = 0.8\n",
    "p_2_hat = 0.74\n",
    "\n",
    "Psi_samples = np.zeros(shape = B)\n",
    "\n",
    "for i in range(0,B): \n",
    "    Psi_samples[i] = np.random.binomial(n_1, p_1_hat)/n_1 - np.random.binomial(n_2, p_2_hat)/n_2\n",
    "\n",
    "Psi_samples = np.sort(Psi_samples)\n",
    "\n",
    "lower_bound = Psi_samples[(int)(0.05*B)]\n",
    "upper_bound = Psi_samples[(int)(0.95*B)]\n",
    "\n",
    "print(\"95% confidence interval using parametric boostrap:\")\n",
    "print(f\"[{lower_bound:.4f},{upper_bound:.4f}]\")\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "plt.plot(Psi_samples)\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-pennsylvania",
   "metadata": {},
   "source": [
    "### Exercise 9.8\n",
    "\n",
    "Done in Exercise 9.3 part (b). \n",
    "\n",
    "### Exercise 9.9\n",
    "\n",
    "(a) <br> \n",
    "\n",
    "<b>Delta Method: </b>\n",
    "\n",
    "$I_n(\\mu) = -\\mathbb{E}_\\mu(\\dfrac{\\partial^2ln(f(x;\\mu))}{\\partial\\mu^2}) = n/\\sigma^2$\n",
    "\n",
    "$\\hat{se}(\\hat\\mu) = 1/\\sqrt{I_n(\\mu)} = \\sigma/\\sqrt{n}$\n",
    "\n",
    "$\\theta = g(\\mu) = exp(\\mu) \\implies g'(\\hat\\mu) = exp(\\hat\\mu)$\n",
    "\n",
    "$\\hat{se}(\\hat\\mu) = |g'(\\hat\\mu)|\\hat{se}(\\hat\\mu) = exp(\\hat\\mu)\\sigma/\\sqrt{n} =  exp(\\bar x)/\\sqrt{n} = exp(\\bar x)/10$\n",
    "\n",
    "$C_n = exp(\\bar x) \\pm \\mathcal{z}_{0.9}\\hat{se}(\\hat\\mu) = exp(\\bar x) \\pm (1.2816)exp(\\bar x)/10$\n",
    "\n",
    "<b>Parametric bootstrap: </b>\n",
    "\n",
    "\n",
    "(b) <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binary-victory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-operations",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "freelance-extension",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
